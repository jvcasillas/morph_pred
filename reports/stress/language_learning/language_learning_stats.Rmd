---
author: 
  - name          : "Nuria Sagarra"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
  - name          : "Cristina Lozano-Argüelles"
    affiliation   : "1"
  - name          : "Joseph V. Casillas"
    affilitation  : "1"

affiliation:
  - id            : "1"
    institution   : "Rutgers University"



keywords          : ""
wordcount         : ""

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : yes
linenumbers       : yes
mask              : yes
draft             : yes

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library("papaja")
library("officer")
library("flextable")
```


```{r, 'source-scripts-models'}
source(here::here("scripts", "02_load_data.R"))

# Get path to saved models
gca_mods_path  <- here("models", "stress", "s3_adv_int_nat", 
                       "eye_track", "gca")

# Load models as list and store full mod to global env
load(paste0(gca_mods_path, "/full_mods_lang_learn.Rdata"))
load(paste0(gca_mods_path, "/ind_mods_wm.Rdata"))
load(paste0(gca_mods_path, "/nested_model_comparisons_wm.Rdata"))
load(paste0(gca_mods_path, "/model_preds_wm.Rdata"))

gca_wm_condition_sum_grp_mod_int_2 <- 
  full_mods_lang_learn$gca_wm_condition_sum_grp_mod_int_2

```






# Language learning article (stress and working memory with natives, late advanced learners and interpreters)


Intercept (γ00)	1.176	0.214	5.497	< .001
Time1 (γ10)	5.421	0.746	7.262	< .001
Time2 (γ20)	−1.372	0.396	−3.464	< .001
Time3 (γ30)	−1.677	0.297	−5.644	< .001
Linear quadratic and cubic time term polynomials captured the functional sigmoid shape of the data



Time1 × Syllable structure (γ11)	0.819	0.375	2.183	.029
Time2 × Lexical stress (γ22)	0.575	0.241	2.386	.017
Time3 × Lexical stress (γ32)	−0.579	0.161	−3.587	< .001
Time3 × Syllable structure × Lexical stress (γ35)	−0.491	0.226	−2.172	.030
Time2 × Lexical stress × Working memory (γ26)	−0.862	0.250	−3.450	< .001
Time1 × Syllable structure × Working memory (γ18)	0.239	0.113	2.107	.035


# Monolingual group

- There was a main effect of syllabic structure on the linear term, indicating that a change from CV to CVC increased the steepness of the slope.
- A main effect of lexical stress was found on the quadratic and cubic time terms. A changed from paroxytone to oxytone increased.... However, 


#############################################################################

# NIN group

Time2 × Group NIN (γ23)	1.823	0.471	3.871	< .001

Time1 × Syllable structure × Lexical stress × Group NIN (γ07)	0.895	0.277	3.229	.001

Time2 × Lexical stress × Group NIN:Working memory (γ010)	0.701	0.325	2.155	.031

#############################################################################

# IN group

Time2 × Group IN (γ24)	1.615	0.480	3.360	< .001

Lexical stress × Group IN × Working memory (γ19)	−0.567	0.261	−2.171	.030

Time3 × Syllable structure × Lexical stress × Group IN (γ39)	0.846	0.277	3.052	.002

Time1 × Lexical stress × Group IN:Working memory (γ37)	0.684	0.301	2.273	.023

Time2 × Lexical stress × Group IN:Working memory (γ210)	0.772	0.299	2.580	.010

############################################################################

# Pairwise comparisons

Lexical stress × IN - NIN × wm_std (γ19)	0.492	0.236	2.086	.037

Time1 × Syllable structure × Lexical stress × IN - NIN (γ08)	1.566	0.279	5.610	< .001

Time3 × Syllable structure × Lexical stress × IN - NIN (γ28)	−0.847	0.276	−3.073	.002



# Plots

```{r, 'plot-ind-preds', echo=FALSE, fig.cap="Growth curve estimates of target fixations as a function of lexical stress and syllable structure and working memory for each group during the analysis window. Lines represent model estimates at -1, 0, and 1 standard deviations of working memory. The transparent ribbons represents ±SE. Empirical logit values on the y-axis correspond to proportions of 0.12, 0.50, 0.88, and 0.98. The thick horizontal white line represents the 50% probability of fixating on the target. The thick vertical white line indicates 200 ms after the offset of the target syllable."}
knitr::include_graphics(
  here("figs", "stress", "s3_adv_int_nat", "eye_track", "lang_learn", 
  "stress_p1.png")
)
```

```{r, 'plot-group-preds', echo=FALSE, fig.cap="Growth curve estimates of target fixations as a function of lexical stress and syllable structure for each group during the analysis window. Symbols and lines represent model estimates at mean working memory, and the transparent ribbons represents ±SE. Empirical logit values on the y-axis correspond to proportions of 0.12, 0.50, 0.88, and 0.98. The thick horizontal white line represents the 50% probability of fixating on the target. The thick vertical white line indicates 200 ms after the offset of the target syllable."}
knitr::include_graphics(
  here("figs", "stress", "s3_adv_int_nat", "eye_track", "lang_learn", 
  "stress_p2.png")
)
```



## Qualitative description - GCA + WM


- Graphs on the left reflect looks towards paroxytone (more common, FIRma) targets 
- Graphs on the right reflect looks towards oxytone (less common, firMÓ) targets
- Red curves are for CV (more common)
- Blue curves are for CVC (less common)
- Solid line indicates participants with lower WM
- Dotted line indicates participants with avareage WM
- Dashed line indicates participants with higher WM



Overall tendencies:
- We see that blue curves are above red curves for all groups in both stress conditions, indicating an overall higher proportion of fixation in CVC than in CV targets.


1. Monolinguals

- For paroxytones (FIRma), we see that monolinguals with higher WM were able to anticipate at a higher rate in both CV and CVC (even higher with CVC) than monolinguals with average and lower WM. It seems like monolinguals with low WM (red solid line) are not anticipating above chance at the offset of the target syllable. WM seems to be an important factor here, there is more space between the lines (solid, dotted, dashed). There seems to be an interaction (CV and CVC curves intersect), but later in the time course??

- For oxytones (firMÓ), we see the opposite pattern. Participants with lower WM (solid line), are anticipating at a higher rate than those with average (dotted line) or higher (dashed line) WM. CVC targets are predicted at a higher rate. Both CV and CVC curves seem to follow the same trajectory. 

2. Non-interpreters

- For paroxytones (FIRma), we see that non-interpreters with higher WM were able to anticipate at a higher rate in both CV and CVC (even higher with CVC) than non-intepreters with average and lower WM. For CV targers, it seems like they are not anticipating, although those with higher WM could be doing it above chance (dashed curve is above chance). There seems to be an interaction (CV and CVC curves intersect).

- For oxytones (firMÓ), we see again the opposite pattern. Non-interpreters with lower WM (solid line), are anticipating at a higher rate than those with average (dotted line) or higher (dashed line) WM. Differences due to WM are smaller (dotted, dashed and solid lines are closer together). Both CV and CVC curves seem to follow the same trajectory.


3. Interpreters

- For paroxytones (FIRma), we see that interpreters with lower WM are anticipating at a higher rate in both CVC and CV conditions. For the CV condition (red line), it seems like they are not anticipating above chance at the offset of the targer syllable. Both CV and CVC curves seem to follow the same trajectory.



- For oxytones (firMÓ), we see the opposite pattern, interpreters with higher WM (dashed line) are anticipating at a higher rate than those with average (dotted line) and lower (solid line) WM, for both CV and CVC. There seems to be an interaction (CV and CVC curves intersect). 

- WM seems to play a smaller role among interpreters, lines are closer together for all conditions. 





# Tables 

## Model estimates at target syllable offset  


```{r, table-target-offset-props, eval=T, echo=F}

border_1 <- fp_border(width = 1.5)
border_2 <- fp_border(width = 0.75)

model_preds_wm$target_offset_wm_preds %>% 
  mutate(coda = if_else(coda == 1, "CV", "CVC"), 
         cond = if_else(cond == 1, "Paroxytone", "Oxytone")) %>% 
  filter(wm == 0) %>% 
  group_by(group, coda, cond) %>% 
  summarize(prob = mean(prob), prob_lb = mean(prob_lb), prob_ub = mean(prob_ub)) %>% 
  ungroup() %>% 
  arrange(group, coda, desc(cond)) %>% 
  mutate(group = blank_same_as_last(as.character(group)), 
         coda =  blank_same_as_last(coda)) %>% 
  select(Group = group, `Lexical stress` = cond, 
         `Syllable structure` = coda, Probability = prob, LB = prob_lb, 
         UB = prob_ub) %>% 
  flextable() %>% 
  width(., j = c(2, 3, 4), width = c(1.1, 1.3, 1.1)) %>% 
  font(., fontname = "Times", part = "all") %>%
  fontsize(., size = 11) %>% 
  border_remove(.) %>%  
  border(., part = "header", 
            border.top = border_1,
            border.bottom = border_2) %>% 
  hline_bottom(., part = "body", border = border_1)
```

*Table 1*: Model estimates at mean working memory for probability of target fixations ±SE at 200 ms after the target syllable offset.




## Fixed effects

```{r eval=T, warning=FALSE}
fixef_order <- c(
1, 2, 3, 4, 5, 9, 13, 17, 6, 7, 8, 10, 11, 12, 21, 14, 18, 15, 19, 16, 20, 25, 28, 22, 23, 24, 30, 34, 26, 27, 34, 36, 38, 28, 35, 37, 39, 32, 40, 42, 33, 41, 43)

pretty_fixed_effects <- gca_wm_condition_sum_grp_mod_int_2 %>% 
  tidy_lme4 %>% 
  mutate(index = fixef_order) %>% 
  arrange(index) %>% 
  select(-index) %>% 
  mutate(p = format_pval(p), 
         Parameter = fix_param_names(Parameter)) %>% 
  mutate_each(funs(format_fixef_num), Estimate:t) %>% 
  rename(`_t_` = t, `_p_` = p) 

# Include gammas (Gij) after each parameter name
subs <- c(paste0(0:3, 0), paste0(0:3, 1), 
          paste0(0:3, 2), paste0(0:3, 3), 
          paste0(0:3, 4), paste0(0:3, 5), 
          paste0(0:3, 6), paste0(0:3, 7), 
          paste0(0:3, 8), paste0(0:3, 9), 
          paste0(0:2, 10))
var_labels <- parenthesize(paste0(emphasize("&gamma;"), "~", subs, "~"))
pretty_fixed_effects$Parameter %<>% paste(., var_labels)
pretty_fixed_effects$Parameter %<>% str_replace("wm_std", "Working memory")

pretty_fixed_effects %>% 
  knitr::kable(format = "pandoc", align = str_tokenize("lrrrr")) 

```

Appendix 1: Growth curve model fixed effects


## Random effects

```{r, 'ranef-table', results = "asis", eval=T}
ranef_table <- gca_wm_condition_sum_grp_mod_int_2 %>% 
  tidy_ranef_summary %>% 
  # Format the numbers
  mutate_each(funs(format_fixef_num), vcov, sdcor) %>%
  mutate_each(funs(format_cor), -var1, -grp, -vcov, -sdcor) %>%
  sort_ranef_grps %>%
  # Format variable names and group names
  mutate(var1 = fix_param_names(var1) %>% blank_nas,
         grp =  blank_same_as_last(grp) %>% fix_param_names) %>% 
  rename(Group = grp, Parameter = var1, Variance = vcov, SD = sdcor)

# Correlation columns need names with characters so that pandoc can align them
names(ranef_table)[5:10] <- 
  c("Correlations", "&nbsp;", " &nbsp;", "  &nbsp;", "  &nbsp;", "  &nbsp;")

ranef_table %>% 
  knitr::kable(format = "pandoc", align = str_tokenize("llrrrrrr"))
```

Appendix 2: Growth curve model random effects




# Statistical Analysis

```{r, 'write-up-prep', warning=F, eval=T}
# Prepare table to support easy in-line printing of equations
params <- pretty_fixed_effects %>% 
  rename(B = Estimate, t = `_t_`, p = `_p_`)
params$B %<>% str_replace("&minus;", "-")
params$SE %<>% str_replace("&minus;", "-")
params$t %<>% str_replace("&minus;", "-")
params$p %<>% str_replace("< ", "") %>% as.numeric

params$subscript <- 
  c(paste0(0:3, 0), paste0(0:3, 1), 
    paste0(0:3, 2), paste0(0:3, 3), 
    paste0(0:3, 4), paste0(0:3, 5), 
    paste0(0:3, 6), paste0(0:3, 7), 
    paste0(0:3, 8), paste0(0:3, 9), 
    paste0(0:2, 10))

params <- tibble::column_to_rownames(params, 'subscript')

params$subscript <- 
  c(paste0(0:3, 0), paste0(0:3, 1), 
    paste0(0:3, 2), paste0(0:3, 3), 
    paste0(0:3, 4), paste0(0:3, 5), 
    paste0(0:3, 6), paste0(0:3, 7), 
    paste0(0:3, 8), paste0(0:3, 9), 
    paste0(0:2, 10))

# Shortcut for inline reporting from the above table
report_row <- function(row_name) report_fixef_row(params, row_name)

# Pre-calculate intercept as proportion
b0 <- params$B[params$subscript == "00"] %>% as.numeric
b0_prop <- b0 %>% inv_logit %>% round(2) %>% remove_leading_zero

```



## Pairwise comparisons

```{r, 'learner-comparison'}
# make table for printing
pairwise_comp <- full_mods_lang_learn$gca_full_mod_wm_relevel %>%
  tidy_lme4 %>%
  rename(B = Estimate) %>%
  filter(str_detect(Parameter, "la") == TRUE)

# Add subscript column, convert to rownames, and then add it again
pairwise_comp$subscript <- c(paste0(0:4, 8), paste0(0:5, 9))
pairwise_comp <- tibble::column_to_rownames(pairwise_comp, 'subscript')
pairwise_comp$subscript <- c(paste0(0:4, 8), paste0(0:5, 9))


# Shortcut for inline reporting from the above table
report_pairwise <- function(row_name) report_fixef_row(pairwise_comp, row_name)

```


```{r, pairwise-comp-table, results='asis', eval=T}

# Make table for appendix
pretty_pairwise_table <- full_mods_lang_learn$gca_full_mod_wm_relevel %>%
  tidy_lme4 %>%
  mutate(p = format_pval(p),
         Parameter = fix_param_names(Parameter)) %>%
  mutate_each(funs(format_fixef_num), Estimate:t) %>%
  rename(`_t_` = t, `_p_` = p) %>%
  filter(str_detect(Parameter, "NIN") == TRUE)

pretty_pairwise_table$Parameter %<>% str_replace("Group NIN", "IN - NIN")

# Include gammas after each parameter name
pw_subs <- c(paste0(0:3, 8), paste0(0:1, 9))
pw_var_labels <- parenthesize(paste0(emphasize("&gamma;"), "~", pw_subs, "~"))
pretty_pairwise_table$Parameter %<>% paste(., pw_var_labels)

pretty_pairwise_table %>%
  knitr::kable(format = "pandoc", align = str_tokenize("lrrrr"))

```

Appendix 3: Pairwise comparisons between learner groups.




Example write up (must be rewritten)  

(Reporting rows from final model)  

Figure 1 plots the model estimates from the GCA, and the full model summary is 
available in Appendices 1 and 2. 
We report the results for the M group and then provide comparisons with and 
between the learner groups. 
The model intercept estimates the log odds of M fixating on the target, 
averaging over the time course, lexical stress, and syllable structure, at the 
mean working memory (XXX). 
The log odds were _&gamma;_~00~&nbsp;=&nbsp;`r fixed_digits(b0, 2)` 
(proportion:&nbsp;`r b0_prop`). 
The linear, quadratic, and cubic polynomial time terms captured the sigmoid 
shape of the time course and were retained in the model 
(`r report_row("10")`; `r report_row("20")`; `r report_row("30")`). 

(Reporting nested model comparisons)  

